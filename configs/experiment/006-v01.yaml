# @package _global_

# to execute this experiment run:
# python train.py experiment=example

# defaults:
#   - override /hogehoge: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

experiment_name: ${hydra:job.override_dirname}
notebook: v01

debug: false
seed: 8825

feature_store: default
cache_feature_extractors: false
overwrite_training: true

cv:
  _target_: sklearn.model_selection.StratifiedGroupKFold
  n_splits: 5
  random_state: 0
  shuffle: true

feature_extractors:
  - _target_: src.experiment.customs.feature.DisbursementDateExtractorV1
  - _target_: src.experiment.customs.feature.ApprovalDateExtractorV1
  - _target_: src.experiment.customs.feature.DisbursementGrossExtractorV1
  - _target_: src.experiment.customs.feature.GrAppvExtractorV1
  - _target_: src.experiment.customs.feature.SBA_AppvExtractorV1
  - _target_: src.experiment.customs.feature.RevLineCrExtractorV1
  - _target_: src.experiment.customs.feature.LowDocExtractorV1
  - _target_: src.experiment.customs.feature.EmployeeMoneyFeatureExtractorV1
  - _target_: src.experiment.customs.feature.UrbanRuralFeatureExtractorV1
  - _target_: src.experiment.customs.feature.StateFeatureExtractorV1
  - _target_: src.experiment.feature.tabular.RawExtractor
    cols:
      - Term
      - NoEmp
      - NewExist
      - CreateJob
      - RetainedJob
      - FranchiseCode
      - Sector
      - ApprovalFY
      - DisbursementGross
      - GrAppv
      - SBA_Appv

  # ordinal encorded features
  - _target_: src.experiment.feature.tabular.OrdinalFeatureExtractor
    input_cols: [City, State, BankState]

agg_feature_extractors:
  # aggregated features
  - _target_: src.experiment.feature.tabular.AggregatedFeatureExtractor
    group_keys: null # set in runtime (use group_keys_for_agg)
    group_values: [
        ## raw features
        Term,
        NoEmp,
        CreateJob,
        RetainedJob,
        DisbursementGross,
        GrAppv,
        SBA_Appv,

        ## engineered features
        # employee
        Create_plus_RetainedJob,
        NoEmpALL,
        CreateJob_pre_logTerm,
        RetainedJob_pre_logTerm,
        CreateJob_pre_NoEmp,
        RetainedJob_pre_NoEmp,
        # money
        DisbursementGross_pre_logTerm,
        DisbursementGross_diff_GrAppv,
        DisbursementGross_diff_SBA_Appv,
        DisbursementGross_ratio_GrAppv,
        DisbursementGross_ratio_SBA_Appv,
        # money + employee
        DisbursementGross_pre_NoEmp,
        DisbursementGross_pre_CreateJob,
        DisbursementGross_pre_RetainedJob,
      ]
    agg_methods: ["std", "mean"]
    extr_agg_methods: [] # [] # ["z-score"]
    transform_method: first
    parents:
      - _target_: src.experiment.customs.feature.EmployeeMoneyFeatureExtractorV1
      - _target_: src.experiment.customs.feature.UrbanRuralFeatureExtractorV1
      - _target_: src.experiment.customs.feature.RevLineCrExtractorV1
      - _target_: src.experiment.customs.feature.LowDocExtractorV1
      - _target_: src.experiment.customs.feature.StateFeatureExtractorV1

group_keys_for_agg: [
    ## raw features
    [City],
    [State],
    [BankState],
    [Sector],
    [ApprovalFY],
    [NewExist],

    ## engineered features
    [LowDoc_label],
    [RevLineCr_dummy],
    [UrbanRural_dummy],
    [same_State_BankState_dummy],

    ## cross features
    [LowDoc_label, RevLineCr_dummy],
    [LowDoc_label, UrbanRural_dummy],
    [LowDoc_label, same_State_BankState_dummy],
    [RevLineCr_dummy, UrbanRural_dummy],
    [RevLineCr_dummy, same_State_BankState_dummy],
    [UrbanRural_dummy, same_State_BankState_dummy],
    [NewExist, LowDoc_label],
    [NewExist, RevLineCr_dummy],
    [NewExist, UrbanRural_dummy],
    [NewExist, same_State_BankState_dummy],
  ]

# change random_state and max_depth (lightgbm)
seed_average_seeds: [3, 5, 7]
model:
  predict_proba: true
  use_eval_set: true
  use_xgb_class_weight: true
  return_binary: true
  estimator: { _target_: xgboost.XGBClassifier }
  eval_metric: src.experiment.model.custom_metrics.xgb_macro_f1
  # https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier
  params:
    n_estimators: 10000
    learning_rate: 0.05
    objective: "binary:logistic" # binary:hinge, binary:logistic
    booster: "gbtree"
    tree_method: "hist"
    max_depth: 8
    colsample_bytree: 0.2
    subsample: 0.5
    random_state: 8823
    importance_type: "gain"
    early_stopping_rounds: 200
    verbosity: 1
  fit_params: {}
