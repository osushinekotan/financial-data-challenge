{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.utils.log_utils import get_consol_handler, get_file_handler\n",
    "from src.experiment.optimization import opt_macro_f1_score\n",
    "from src.experiment.model.custom_metrics import (\n",
    "    lgb_macro_f1,\n",
    "    xgb_macro_f1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=000-v01\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "# set directories as global variables\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.paths.output_dir = f\"{CFG.paths.output_dir}_debug\"\n",
    "\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [\n",
    "    get_file_handler(OUTPUT_DIR / \"notebook.log\"),\n",
    "    get_consol_handler(),\n",
    "]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"MIS_Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_feature_extractors(feature_extractors, all_group_keys):\n",
    "    if feature_extractors is None:\n",
    "        return []\n",
    "\n",
    "    if all_group_keys is None:\n",
    "        return []\n",
    "\n",
    "    extractors = []\n",
    "    for extractor in feature_extractors:\n",
    "        for group_keys in all_group_keys:\n",
    "            _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "            extractors.append(_extractor)\n",
    "    return extractors\n",
    "\n",
    "\n",
    "# train features : train data ã®ã¿ã‹ã‚‰ä½œæˆ\n",
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_agg\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"te_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_te\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"rolling_agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_rolling_agg\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "train_feature_df = run_extractors(\n",
    "    input_df=train_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "train_feature_df = pd.concat([train_df, train_feature_df], axis=1)\n",
    "\n",
    "# test features : test data ã‹ã‚‰ä½œæˆ\n",
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_agg\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"te_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_te\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"rolling_agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_rolling_agg\"),\n",
    "    )\n",
    ")\n",
    "test_feature_df = run_extractors(\n",
    "    input_df=test_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=False,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "\n",
    "test_feature_df = pd.concat([test_df, test_feature_df], axis=1)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]\n",
    "logger.info(f\"train_feature_df.shape: {train_feature_df.shape}\")\n",
    "logger.info(f\"test_feature_df.shape: {test_feature_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_seed_average_pred(result_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [col for col in result_df.columns if col.startswith(\"pred_\")]\n",
    "    pred = 0\n",
    "    for col in cols:\n",
    "        pred += np.array(result_df[col].tolist())\n",
    "    result_df[\"pred\"] = list(pred / len(cols))\n",
    "    return result_df.reset_index()\n",
    "\n",
    "\n",
    "valid_result_df = pd.DataFrame()\n",
    "all_trained_estimators = []\n",
    "scores = {}\n",
    "for seed in CFG.seed_average_seeds:\n",
    "    logger.info(f\"\\n\\nstart training seed={seed} ðŸš€\")\n",
    "\n",
    "    CFG.model.estimator.random_state = seed\n",
    "\n",
    "    fit_params = dict(hydra.utils.instantiate(CFG.model.fit_params))\n",
    "    if CFG.model.estimator._target_.startswith(\"lightgbm.LGBM\"):\n",
    "        CFG.model.estimator.num_leaves = seed  # lgbm\n",
    "        fit_params[\"eval_metric\"] = [lgb_macro_f1]\n",
    "\n",
    "    estimator = hydra.utils.instantiate(CFG.model.estimator)\n",
    "\n",
    "    if CFG.model.estimator._target_.startswith(\"xgboost.XGB\"):\n",
    "        params = estimator.get_params()\n",
    "        params[\"eval_metric\"] = xgb_macro_f1\n",
    "        params[\"max_depth\"] = seed\n",
    "        estimator.set_params(**params)  # xgb\n",
    "\n",
    "    model_output_dir = OUTPUT_DIR / \"models\" / f\"seed{seed}\"\n",
    "    trained_estimators = train_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimator=estimator,\n",
    "        feature_columns=feature_columns,\n",
    "        target_columns=[\"MIS_Status\"],\n",
    "        fit_params=fit_params,\n",
    "        output_dir=model_output_dir,\n",
    "        overwrite=CFG.overwrite_training,\n",
    "        use_xgb_class_weight=CFG.model.get(\"use_xgb_class_weight\"),\n",
    "    )\n",
    "\n",
    "    i_valid_result_df = predict_cv_tabular_v1(\n",
    "        df=train_feature_df.query(\"data == 'train'\").reset_index(drop=True),\n",
    "        estimators=trained_estimators,\n",
    "        feature_columns=feature_columns,\n",
    "        predict_proba=CFG.model.predict_proba,\n",
    "    )\n",
    "    val_score = f1_score(\n",
    "        y_true=i_valid_result_df[\"MIS_Status\"],\n",
    "        y_pred=i_valid_result_df[\"pred\"].tolist(),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    logger.info(f\"macro f1 score [seed={seed}]: {val_score}\")\n",
    "    scores[f\"seed{seed}\"] = val_score\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [\n",
    "            valid_result_df,\n",
    "            i_valid_result_df[[\"uid\", \"pred\", \"MIS_Status\"]]\n",
    "            .set_index([\"uid\", \"MIS_Status\"])\n",
    "            .rename(columns={\"pred\": f\"pred_{seed}\"}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    all_trained_estimators.extend(trained_estimators)\n",
    "\n",
    "\n",
    "valid_result_df = assign_seed_average_pred(valid_result_df)\n",
    "val_proba = np.array(valid_result_df[\"pred\"].tolist()).reshape(-1)\n",
    "opt_result: dict = opt_macro_f1_score(\n",
    "    y_true=valid_result_df[\"MIS_Status\"].to_numpy(),\n",
    "    y_pred=val_proba,\n",
    ")\n",
    "val_pred_label = val_proba >= opt_result[\"th\"]\n",
    "\n",
    "scores[\"all_normal\"] = f1_score(\n",
    "    y_true=valid_result_df[\"MIS_Status\"], y_pred=val_proba, average=\"macro\"\n",
    ")\n",
    "scores[\"all_opt\"] = opt_result[\"score\"]\n",
    "logger.info(f\"score: {scores}\")\n",
    "\n",
    "joblib.dump(\n",
    "    valid_result_df[[\"uid\", \"MIS_Status\", \"pred\"]], OUTPUT_DIR / \"valid_result_df.pkl\"\n",
    ")\n",
    "json.dump(scores, open(OUTPUT_DIR / \"scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")\n",
    "fig.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "importance_df.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n",
    "\n",
    "\n",
    "fig = plot_label_distributions(proba_matrix=np.array(valid_result_df[\"pred\"].tolist()))\n",
    "fig.show()\n",
    "fig.savefig(OUTPUT_DIR / \"label_distributions.png\", dpi=300)\n",
    "\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"MIS_Status\"], y_pred=val_pred_label)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(\n",
    "    y_true=valid_result_df[\"MIS_Status\"], y_pred=val_pred_label, normalize=True\n",
    ")\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    "    predict_proba=CFG.model.predict_proba,\n",
    ")\n",
    "\n",
    "test_proba = np.array(test_result_df[\"pred\"].tolist()).reshape(-1)\n",
    "test_pred_df = test_result_df[[\"uid\"]].asgin(pred=test_proba).groupby(\"uid\").mean().reset_index()\n",
    "test_pred_df[\"pred_label\"] = test_pred_df[\"pred\"] >= opt_result[\"th\"]\n",
    "\n",
    "submission_df = test_df[[\"uid\"]].merge(test_pred_df[[\"uid\", \"pred_label\"]], on=\"uid\", how=\"left\")\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_{scores['all_opt']:.3f}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
