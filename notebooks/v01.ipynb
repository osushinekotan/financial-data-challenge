{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import omegaconf\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    "    plot_venn_diagrams,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.utils.log_utils import get_consol_handler, get_file_handler\n",
    "from src.experiment.optimization import opt_macro_f1_score\n",
    "from src.experiment.model.custom_metrics import lgb_macro_f1, xgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=003-v01\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = hydra.compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "# set directories as global variables\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.paths.output_dir = f\"{CFG.paths.output_dir}_debug\"\n",
    "\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BASE_OUTPUT_DIR = Path(CFG.paths.resource_dir) / \"outputs\"\n",
    "\n",
    "\n",
    "# set logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [\n",
    "    get_file_handler(OUTPUT_DIR / \"notebook.log\"),\n",
    "    get_consol_handler(),\n",
    "]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    df[\"group\"] = df[\"State\"] + \"_\" + df[\"FranchiseCode\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def money_to_numeric(x: str) -> int:\n",
    "    return int(x.replace(\"$\", \"\").replace(\",\", \"\").split(\".\")[0])\n",
    "\n",
    "\n",
    "def transform_money(input_df: pd.DataFrame):\n",
    "    df = input_df.copy()\n",
    "    df[\"DisbursementGross\"] = df[\"DisbursementGross\"].map(money_to_numeric, na_action=\"ignore\")\n",
    "    df[\"GrAppv\"] = df[\"GrAppv\"].map(money_to_numeric, na_action=\"ignore\")\n",
    "    df[\"SBA_Appv\"] = df[\"SBA_Appv\"].map(money_to_numeric, na_action=\"ignore\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")\n",
    "\n",
    "train_df = transform_money(train_df)\n",
    "test_df = transform_money(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label distribution\n",
    "plot_venn_diagrams(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    cat_cols=[\n",
    "        \"State\",\n",
    "        \"BankState\",\n",
    "        \"Sector\",\n",
    "        \"FranchiseCode\",\n",
    "        \"UrbanRural\",\n",
    "        \"group\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"MIS_Status\", group_col=\"group\")\n",
    "\n",
    "if CFG.debug:\n",
    "    train_df_neg = train_df.query(\"MIS_Status == 0\").sample(1000, random_state=CFG.seed)\n",
    "    train_df_pos = train_df.query(\"MIS_Status == 1\").sample(1000, random_state=CFG.seed)\n",
    "    train_df = pd.concat([train_df_neg, train_df_pos]).reset_index(drop=True)\n",
    "\n",
    "train_df.groupby(\"fold\")[\"MIS_Status\"].agg([\"count\", \"mean\", \"sum\"]).assign(\n",
    "    negative=lambda x: x[\"count\"] - x[\"sum\"], ratio=lambda x: x[\"sum\"] / x[\"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_exps = CFG.get(\"ensemble_exps\")\n",
    "if ensemble_exps is not None:\n",
    "    for exp in ensemble_exps:\n",
    "        logger.info(f\"Load {exp} ...\")\n",
    "        train_filepath = BASE_OUTPUT_DIR / exp / \"valid_result_df.pkl\"\n",
    "        test_filepath = BASE_OUTPUT_DIR / exp / \"test_result_df.pkl\"\n",
    "\n",
    "        train_result_df = (\n",
    "            joblib.load(train_filepath)[[\"uid\", \"pred\", \"pred_label\"]].set_index(\"uid\").add_prefix(f\"f_{exp}_\")\n",
    "        )\n",
    "        test_result_df = (\n",
    "            joblib.load(test_filepath)[[\"uid\", \"pred\", \"pred_label\"]].set_index(\"uid\").add_prefix(f\"f_{exp}_\")\n",
    "        )\n",
    "        train_df = train_df.merge(train_result_df, on=\"uid\", how=\"left\")\n",
    "        test_df = test_df.merge(test_result_df, on=\"uid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_feature_extractors(feature_extractors, all_group_keys):\n",
    "    if feature_extractors is None:\n",
    "        return []\n",
    "\n",
    "    if all_group_keys is None:\n",
    "        return []\n",
    "\n",
    "    extractors = []\n",
    "    for extractor in feature_extractors:\n",
    "        for group_keys in all_group_keys:\n",
    "            _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "            extractors.append(_extractor)\n",
    "    return extractors\n",
    "\n",
    "\n",
    "# train features : train data のみから作成\n",
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_agg\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"te_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_te\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"rolling_agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_rolling_agg\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "train_feature_df = run_extractors(\n",
    "    input_df=train_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "train_feature_df = pd.concat([train_df, train_feature_df], axis=1)\n",
    "\n",
    "# test features : test data から作成\n",
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_agg\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"te_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_te\"),\n",
    "    )\n",
    ")\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(\n",
    "        feature_extractors=CFG.get(\"rolling_agg_feature_extractors\"),\n",
    "        all_group_keys=CFG.get(\"group_keys_for_rolling_agg\"),\n",
    "    )\n",
    ")\n",
    "test_feature_df = run_extractors(\n",
    "    input_df=test_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=False,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "\n",
    "test_feature_df = pd.concat([test_df, test_feature_df], axis=1)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]\n",
    "logger.info(f\"train_feature_df.shape: {train_feature_df.shape}\")\n",
    "logger.info(f\"test_feature_df.shape: {test_feature_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_seed_average_pred(result_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [col for col in result_df.columns if col.startswith(\"pred_\")]\n",
    "    pred = 0\n",
    "    for col in cols:\n",
    "        pred += np.array(result_df[col].tolist())\n",
    "    result_df[\"pred\"] = list(pred / len(cols))\n",
    "    return result_df.reset_index()\n",
    "\n",
    "\n",
    "def to_python_type(value):\n",
    "    if isinstance(value, omegaconf.DictConfig):\n",
    "        return dict(value)\n",
    "    elif isinstance(value, omegaconf.ListConfig):\n",
    "        return list(value)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "valid_result_df = pd.DataFrame()\n",
    "all_trained_estimators = []\n",
    "scores = {}\n",
    "for seed in CFG.seed_average_seeds:\n",
    "    logger.info(f\"\\n\\nstart training seed={seed} 🚀\")\n",
    "\n",
    "    fit_params = dict(hydra.utils.instantiate(CFG.model.fit_params))\n",
    "    params = {k: to_python_type(v) for k, v in CFG.model.get(\"params\").items()}\n",
    "    if CFG.model.estimator._target_.startswith(\"lightgbm.LGBM\"):\n",
    "        params[\"random_state\"] = seed\n",
    "        params[\"num_leaves\"] = seed  # lgbm\n",
    "        fit_params[\"eval_metric\"] = [lgb_macro_f1]\n",
    "        # NOTE : class_weight などが omegaconf.DictConfig の場合 error になるため set_params で設定\n",
    "\n",
    "    if CFG.model.estimator._target_.startswith(\"xgboost.XGB\"):\n",
    "        params[\"eval_metric\"] = xgb_macro_f1\n",
    "        params[\"max_depth\"] = seed\n",
    "\n",
    "    estimator = hydra.utils.instantiate(CFG.model.estimator).set_params(**params)\n",
    "\n",
    "    model_output_dir = OUTPUT_DIR / \"models\" / f\"seed{seed}\"\n",
    "    trained_estimators = train_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimator=estimator,\n",
    "        feature_columns=feature_columns,\n",
    "        target_columns=[\"MIS_Status\"],\n",
    "        fit_params=fit_params,\n",
    "        output_dir=model_output_dir,\n",
    "        overwrite=CFG.overwrite_training,\n",
    "        use_xgb_class_weight=CFG.model.get(\"use_xgb_class_weight\"),\n",
    "        use_eval_set=CFG.model.get(\"use_eval_set\"),\n",
    "    )\n",
    "\n",
    "    i_valid_result_df = predict_cv_tabular_v1(\n",
    "        df=train_feature_df.query(\"data == 'train'\").reset_index(drop=True),\n",
    "        estimators=trained_estimators,\n",
    "        feature_columns=feature_columns,\n",
    "        predict_proba=CFG.model.get(\"predict_proba\"),\n",
    "    )\n",
    "    i_opt_result: dict = opt_macro_f1_score(\n",
    "        y_true=i_valid_result_df[\"MIS_Status\"].to_numpy(),\n",
    "        y_pred=np.array(i_valid_result_df[\"pred\"].tolist()),\n",
    "    )\n",
    "\n",
    "    logger.info(f\"macro f1 score [seed={seed}]: {i_opt_result}\")\n",
    "    scores[f\"seed{seed}\"] = i_opt_result\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [\n",
    "            valid_result_df,\n",
    "            i_valid_result_df[[\"uid\", \"pred\", \"MIS_Status\"]]\n",
    "            .set_index([\"uid\", \"MIS_Status\"])\n",
    "            .rename(columns={\"pred\": f\"pred_{seed}\"}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    all_trained_estimators.extend(trained_estimators)\n",
    "\n",
    "\n",
    "valid_result_df = assign_seed_average_pred(valid_result_df)\n",
    "val_proba = np.array(valid_result_df[\"pred\"].tolist()).reshape(-1)\n",
    "opt_result: dict = opt_macro_f1_score(\n",
    "    y_true=valid_result_df[\"MIS_Status\"].to_numpy(),\n",
    "    y_pred=val_proba,\n",
    ")\n",
    "val_pred_label = val_proba >= opt_result[\"th\"]\n",
    "scores[\"all_opt\"] = opt_result\n",
    "logger.info(f\"score: {scores}\")\n",
    "\n",
    "joblib.dump(\n",
    "    valid_result_df[[\"uid\", \"MIS_Status\", \"pred\"]].assign(pred_label=val_pred_label * 1),\n",
    "    OUTPUT_DIR / \"valid_result_df.pkl\",\n",
    ")\n",
    "json.dump(scores, open(OUTPUT_DIR / \"scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig, importance_df = visualize_feature_importance(\n",
    "        estimators=all_trained_estimators,\n",
    "        feature_columns=feature_columns,\n",
    "        top_n=50,\n",
    "    )\n",
    "    fig.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "    importance_df.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n",
    "except AttributeError as e:\n",
    "    logger.warning(f\"feature_importance plot failed: {e}\")\n",
    "\n",
    "\n",
    "fig = plot_label_distributions(proba_matrix=np.array(valid_result_df[\"pred\"].tolist()).reshape(-1, 1))\n",
    "fig.show()\n",
    "fig.savefig(OUTPUT_DIR / \"label_distributions.png\", dpi=300)\n",
    "\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"MIS_Status\"], y_pred=val_pred_label)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"MIS_Status\"], y_pred=val_pred_label, normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    "    predict_proba=CFG.model.predict_proba,\n",
    ")\n",
    "\n",
    "test_proba = np.array(test_result_df[\"pred\"].tolist()).reshape(-1)\n",
    "test_pred_df = test_result_df[[\"uid\"]].assign(pred=test_proba).groupby(\"uid\").mean().reset_index()\n",
    "test_pred_df[\"pred_label\"] = (test_pred_df[\"pred\"] >= opt_result[\"th\"]) * 1\n",
    "\n",
    "submission_df = test_df[[\"uid\"]].merge(test_pred_df[[\"uid\", \"pred_label\"]], on=\"uid\", how=\"left\")\n",
    "submission_filepath = (\n",
    "    Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_{scores['all_opt']['score']:.3f}.csv\"\n",
    ")\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)\n",
    "joblib.dump(test_pred_df, OUTPUT_DIR / \"test_result_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
